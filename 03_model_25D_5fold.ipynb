{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 03_model_25D_5fold.ipynb\n",
        "\n",
        "This script contain the model training (using the training set) and making predictions on the independent test set. The model is trained by applying 5-fold cross validation.\n",
        "\n",
        "Required :\n",
        "- (suggest) use of GPU\n",
        "- the 2.5D images in JPEG (.jpeg) format\n",
        "- `local_label.csv`\n",
        "\n",
        "Generate:\n",
        "- the model structure and weight (.h5)\n",
        "- prediction result on the training-validation set (`trainval_5fold_df.csv`)\n",
        "- prediction result on the independent test set (`test_5fold_df.csv`)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from sklearn.model_selection import GroupShuffleSplit, GroupKFold\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 20)\n",
        "pd.set_option(\"display.max_colwidth\", 150)\n",
        "\n",
        "import tensorflow.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.experimental import CosineDecay\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# utilitiy files located in the src folder\n",
        "from src.util_data import check_corrupted, define_classes, find_binary_class_weights\n",
        "from src.util_plot import show_one_image, show_augmentation_layers\n",
        "from src.util_model import get_data_augmentation_layers_keras, get_imgaug_augmentation\n",
        "from src.util_model import load_image_and_label_from_path\n",
        "from src.util_model import create_model_enb1"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1665941831815
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check if TensorFlow is running on GPU"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib \n",
        "# print(device_lib.list_local_devices())\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665941831902
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output (for running on a single GPU):\n",
        "```\n",
        "Num GPUs Available:  1\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# locating the folder paths used in this script.\n",
        "work_dir = <PATH TO project_dir> # path to the directory of the current project (project master path)\n",
        "data_folder_path = work_dir + \"CT_data/\" # where the data is stored\n",
        "image_folder_path = data_folder_path + \"CT_25DJPG_cropped_combineLR_downsampled/\" # directory of the 2.5D images that generated by 02_NIFTI_to_25DJPG.ipynb \n",
        "\n",
        "\n",
        "# load the binary label and add the full file path of the 2.5D JPEG files\n",
        "df_label = pd.read_csv(image_folder_path + \"local_label.csv\")\n",
        "df_label[\"filepath\"] = image_folder_path\n",
        "df_label[\"filepath\"] = df_label[\"filepath\"].str.cat(df_label[\"jpeg_name\"],sep=\"\")\n",
        "\n",
        "display(df_label)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665941832080
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Double check if there are any corrupted file path\n",
        "drop_i_row = check_corrupted(df_label,\"filepath\")\n",
        "df_label = df_label.drop(drop_i_row) "
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665941832142
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display samples\n",
        "1 normal adrenal, and 1 abnormal adrenal."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examples of whole lung vs others. Coronal slice in current path.\n",
        "print(\"Abnormal adrenal:\")\n",
        "image_path = df_label.filepath[df_label.abnormal==1].iloc[10] # showing the 10th image (w.r.t the DataFrame df_label) that has the abnormal flag = 1 (abnormal example).\n",
        "show_one_image(image_path)\n",
        "\n",
        "print(\"Normal adrenal:\")\n",
        "image_path = df_label.filepath[df_label.abnormal==0].iloc[10] # showing the 10th image (w.r.t the DataFrame df_label) that has the abnormal flag = 0 (normal example).\n",
        "show_one_image(image_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665941832739
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input the parameters for the model training"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################## INPUT VALUES ##########################\n",
        "# image data related parameters\n",
        "batch_size = 64 # number of images contained in one batch\n",
        "image_size_h = 240 # input image size (height)\n",
        "image_size_w = 120 # input image size (width)\n",
        "\n",
        "# image augmentation related parameter\n",
        "aug_rotation = (-0.05,0.05) # random rotation\n",
        "aug_zoom = (-0.2, 0) # random zoom\n",
        "aug_contrast = (0.2,0.2) # random contrast\n",
        "aug_coarsedropout = (0.0, 0.10) # random coarse dropout\n",
        "aug_invert = 0.5 # random invert\n",
        "\n",
        "# path and name for the model weight to save \n",
        "set_model_tag = \"ENB1\" # ENB1 = EfficientNet version B1\n",
        "set_model_path = <CURRENT DIRECTORY> + \"/model_h5/\" # set the folder for the model and its weight (.h5) to save\n",
        "\n",
        "# model training related parameters\n",
        "epochs = 8 # number of epochs\n",
        "dropout_rate = 0.2 # droupout rate of the model\n",
        "initial_learning_rate = 1e-4 # initial learning rate\n",
        "alpha = 0.3 # alpha value\n",
        "################################################################\n",
        "\n",
        "input_shape = (image_size_h, image_size_w, 3)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665941832814
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of images and patient from the patient list\n",
        "patient_list = df_label['PatientName'].tolist()\n",
        "print('Number of 2.5D images = ', len(patient_list))\n",
        "patient_list = list(dict.fromkeys(patient_list))\n",
        "print('Number of patient = ', len(patient_list))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665941832930
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spliting the dataset (training and testing)\n",
        "Dataset split into training set and independent test set with a ratio of $80\\%:20\\%$. The split is in unit of patients. Any patient can only be allocated in rather training or test set (never appear in both set). The training set is used for the 5-fold cross validation."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data split (train and test set)\n",
        "test_size=0.2 # set the test set ratio\n",
        "\n",
        "# split data into training set (train_val_df) and test set (test_df).\n",
        "gss = GroupShuffleSplit(n_splits=2, test_size=test_size, random_state=5)\n",
        "train_val_id, test_id = next(gss.split(df_label, groups=df_label['PatientName']))\n",
        "train_val_df = df_label.iloc[train_val_id]\n",
        "test_df = df_label.iloc[test_id]\n",
        "\n",
        "# print the number count in each set\n",
        "print('Number of 2.5D images in the train-val set: ',len(train_val_df))\n",
        "print('Number of 3D images in the train-val set:   ',len(train_val_df.image_name.value_counts()))\n",
        "print('Number of patients in the train-val set:    ', len(train_val_df.PatientName.value_counts()))\n",
        "print('--------------------------------------------------')\n",
        "print('Number of 2.5D images in the test set:      ',len(test_df))\n",
        "print('Number of 3D images in the test set:        ',len(test_df.image_name.value_counts()))\n",
        "print('Number of patients in the test set:         ', len(test_df.PatientName.value_counts()))\n",
        "print('--------------------------------------------------')\n",
        "\n",
        "# double check if there's any patient is allocated in both the train and test set.\n",
        "set_A = set(train_val_id)\n",
        "set_B = set(test_id)\n",
        "# 'Same element!' if there is any element is the same\n",
        "# 'No same element.' if there is no element is the same\n",
        "output = 'Good! No same element.' if (set_A.intersection(set_B) == set()) else 'Same element! Please double check.'\n",
        "print(output)\n",
        "#if True (there is any element same), show which one are in both sets.\n",
        "print([i for i in set_B if i in set_A])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665941832995
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "```\n",
        "Number of 2.5D images in the train-val set:  x\n",
        "Number of 3D images in the train-val set:    y\n",
        "Number of patients in the train-val set:     z\n",
        "--------------------------------------------------\n",
        "Number of 2.5D images in the test set:       X\n",
        "Number of 3D images in the test set:         Y\n",
        "Number of patients in the test set:          Z\n",
        "--------------------------------------------------\n",
        "Good! No same element.\n",
        "[]\n",
        "```\n",
        "Note that the statement `Good! No same element` is important to make sure that there no single patient allocated into both training and test set. There are risks of data leaking (to/from the test set) if the above statement is not printed. If there are any samples allocated into both training and test set, they will be listed (if the list is empty: `[]`, which means no samples are allocated repeatedly)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image augmentation"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the image augmentation layers\n",
        "data_augmentation_layers = get_data_augmentation_layers_keras(aug_rotation=aug_rotation, aug_zoom=aug_zoom, aug_contrast=aug_contrast)\n",
        "seq = get_imgaug_augmentation(aug_coarsedropout, aug_invert)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665941833576
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_batch(image, label):\n",
        "    \"\"\"\n",
        "    Function to apply the augmentation to the batch.\n",
        "    \"\"\"\n",
        "    def augment_image(image):\n",
        "        return seq.augment(image=image.numpy())\n",
        "    image = tf.cast(image, tf.uint8)\n",
        "    image = tf.py_function(augment_image, [image], tf.uint8)\n",
        "    return image, label\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665941833656
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demonstrate the effect of the augmentation layers (keras)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = df_label.filepath.iloc[46]\n",
        "show_augmentation_layers(image_path,data_augmentation_layers)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665941834594
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the model and training"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the classes and get number of classes\n",
        "classes_to_predict, num_classes = define_classes(train_val_df.abnormal)\n",
        "\n",
        "# set the K-Fold for the training set\n",
        "k_fold = 5 # number of fold to train. 5 = 5-fold.\n",
        "gkf = GroupKFold(n_splits=k_fold)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665941834694
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View model structure (using EfficientNet B1)\n",
        "model_efficientnetb1 = create_model_enb1(input_shape=input_shape,\n",
        "                                         num_classes=num_classes,\n",
        "                                         dropout_rate=dropout_rate,\n",
        "                                         data_augmentation_layers=data_augmentation_layers)\n",
        "#print the model structure summary\n",
        "model_efficientnetb1.summary()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665941839497
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output:\n",
        "```\n",
        "Model: \"functional_1\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "input_1 (InputLayer)         [(None, 240, 120, 3)]     0         \n",
        "_________________________________________________________________\n",
        "sequential (Sequential)      (None, None, None, 3)     0         \n",
        "_________________________________________________________________\n",
        "efficientnetb1 (Functional)  (None, 8, 4, 1280)        6575239   \n",
        "_________________________________________________________________\n",
        "global_average_pooling2d (Gl (None, 1280)              0         \n",
        "_________________________________________________________________\n",
        "dropout (Dropout)            (None, 1280)              0         \n",
        "_________________________________________________________________\n",
        "dense (Dense)                (None, 2)                 2562      \n",
        "=================================================================\n",
        "Total params: 6,577,801\n",
        "Trainable params: 6,515,746\n",
        "Non-trainable params: 62,055\n",
        "_________________________________________________________________\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup list to save the training history\n",
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "HISTORY_HISTORY_ACCURACY = []\n",
        "HISTORY_HISTORY_VALACCURACY = []\n",
        "HISTORY_HISTORY_LOSS = []\n",
        "HISTORY_HISTORY_VALLOSS = []"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training (5-fold cross validation)\n",
        "\n",
        "For each fold, the training and validation set split is in term of patient (similar to the train and test split) to avoid data leak."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# starting from fold-1\n",
        "fold_var = 1\n",
        "\n",
        "# 5-fold cross validation\n",
        "for train_index, val_index in gkf.split(train_val_df, groups=train_val_df['PatientName']):\n",
        "    print(\"Fold - \",fold_var)\n",
        "    # set name for the model and weight (.h5) for this fold.\n",
        "    this_model_path = set_model_path+\"model_fold\"+str(fold_var)+\".h5\"\n",
        "    \n",
        "    # allocate train and val set according to the split indix\n",
        "    train_df = train_val_df.iloc[train_index] # training set\n",
        "    val_df = train_val_df.iloc[val_index] # validation set\n",
        "\n",
        "    # get the file (X: in file path as type of string) and the label (y).\n",
        "    X_train = train_df.filepath\n",
        "    y_train = train_df.abnormal\n",
        "    X_val = val_df.filepath\n",
        "    y_val = val_df.abnormal\n",
        "\n",
        "    # find class weights for training set in this fold\n",
        "    class_weight = find_binary_class_weights(y_train)\n",
        "\n",
        "    # build the dataset from the file paths and the labels\n",
        "    training_data = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "    validation_data = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "    training_data = training_data.map(load_image_and_label_from_path, num_parallel_calls=AUTOTUNE)\n",
        "    validation_data = validation_data.map(load_image_and_label_from_path, num_parallel_calls=AUTOTUNE)\n",
        "    training_data = training_data.map(augment_batch) # augmentation\n",
        "\n",
        "    # create the batches\n",
        "    training_data_batches = training_data.shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n",
        "    validation_data_batches = validation_data.shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    # import the model\n",
        "    model = model_efficientnetb1\n",
        "    # set the optimizer and callback\n",
        "    decay_steps = int(round(len(X_train)/batch_size))*epochs\n",
        "    cosine_decay = CosineDecay(initial_learning_rate=initial_learning_rate, decay_steps=decay_steps, alpha=alpha)\n",
        "    callbacks = [ModelCheckpoint(filepath=this_model_path, monitor='val_loss', save_best_only=True)]\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(cosine_decay), metrics=[\"accuracy\"])\n",
        "\n",
        "    # train the model\n",
        "    history = model.fit(training_data_batches,\n",
        "                        epochs = epochs, \n",
        "                        validation_data=validation_data_batches,\n",
        "                        class_weight=class_weight,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "    # save the whole model (structure and weight)\n",
        "    model.save(this_model_path)\n",
        "\n",
        "    # update the model training history\n",
        "    HISTORY_HISTORY_ACCURACY.append(history.history['accuracy'])\n",
        "    HISTORY_HISTORY_VALACCURACY.append(history.history['val_accuracy'])\n",
        "    HISTORY_HISTORY_LOSS.append(history.history['loss'])\n",
        "    HISTORY_HISTORY_VALLOSS.append(history.history['val_loss'])\n",
        "\n",
        "    # load model to evaluate the performance of the model on the validation set in this fold.\n",
        "    model = load_model(this_model_path)\n",
        "\n",
        "    print(\"Evaluate\")\n",
        "    results = model.evaluate(validation_data_batches)\n",
        "    results = dict(zip(model.metrics_names,results))\n",
        "\n",
        "    # update the model validation history\n",
        "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
        "    VALIDAITON_LOSS.append(results['loss'])\n",
        "\n",
        "    # print the validation performance\n",
        "    print(\"Validation accuracy = \",results['accuracy'])\n",
        "    print(\"Validation loss = \",results['loss'])\n",
        "    \n",
        "    # go to next fold\n",
        "    fold_var += 1\n",
        "    print(\"---------------------------------------------------------------\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665948213874
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the training and validation history for record.\n",
        "print(\"HISTORY_HISTORY_ACCURACY = \",HISTORY_HISTORY_ACCURACY)\n",
        "print(\"HISTORY_HISTORY_VALACCURACY = \",HISTORY_HISTORY_VALACCURACY)\n",
        "print(\"HISTORY_HISTORY_LOSS = \",HISTORY_HISTORY_LOSS)\n",
        "print(\"HISTORY_HISTORY_VALLOSS = \",HISTORY_HISTORY_VALLOSS)\n",
        "print(\"VALIDATION_ACCURACY = \",VALIDATION_ACCURACY)\n",
        "print(\"VALIDAITON_LOSS = \",VALIDAITON_LOSS)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665948227172
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# locate the 5 models in the 5-fold cross validation model\n",
        "model_fold1_path = set_model_path + 'model_fold1.h5'\n",
        "model_fold2_path = set_model_path + 'model_fold2.h5'\n",
        "model_fold3_path = set_model_path + 'model_fold3.h5'\n",
        "model_fold4_path = set_model_path + 'model_fold4.h5'\n",
        "model_fold5_path = set_model_path + 'model_fold5.h5'\n",
        "model_path_list = [model_fold1_path,model_fold2_path,model_fold3_path,model_fold4_path,model_fold5_path]"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665948961068
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making and recording the predictions on the train-validation set"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build the training-validation set (without shuffling)\n",
        "X_trainval = train_val_df.filepath\n",
        "y_trainval = train_val_df.abnormal\n",
        "trainval_data = tf.data.Dataset.from_tensor_slices((X_trainval, y_trainval))\n",
        "trainval_data = trainval_data.map(load_image_and_label_from_path, num_parallel_calls=AUTOTUNE)\n",
        "trainval_data_batches = trainval_data.batch(batch_size).prefetch(buffer_size=AUTOTUNE)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# recording the model predictions on the training-validation set for each fold model\n",
        "trainval_result_df = train_val_df\n",
        "for current_model_path in model_path_list:\n",
        "    model_current = load_model(current_model_path)\n",
        "    # Classification on the train-val set\n",
        "    test_predictions = model_current.predict(trainval_data_batches,verbose=1)\n",
        "    # Get most likely class\n",
        "    predicted_classes = np.argmax(test_predictions, axis=1)\n",
        "    current_model_basename = os.path.basename(current_model_path)\n",
        "    current_model_name = os.path.splitext(current_model_basename)[0]\n",
        "    trainval_result_df[\"test_prediction_\"+current_model_name] = test_predictions[:,1]\n",
        "    trainval_result_df[\"predicted_classes_\"+current_model_name] = predicted_classes"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the 5 models predictions on the training-validation set from the 5-fold model\n",
        "trainval_result_df.to_csv(\"trainval_5fold_df.csv\",index=False)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making and recording predictions on the independent test set"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build the test set\n",
        "X_test = test_df.filepath\n",
        "y_test = test_df.abnormal\n",
        "test_data = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "test_data = test_data.map(load_image_and_label_from_path, num_parallel_calls=AUTOTUNE)\n",
        "test_data_batches = test_data.batch(batch_size).prefetch(buffer_size=AUTOTUNE)"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665948505254
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# recording the model predictions on the test set for each fold model\n",
        "test_result_df = test_df\n",
        "for current_model_path in model_path_list:\n",
        "    model_current = load_model(current_model_path)\n",
        "    # make predictions on the test set\n",
        "    test_predictions = model_current.predict(test_data_batches,verbose=1)\n",
        "    # Get most likely class\n",
        "    predicted_classes = np.argmax(test_predictions, axis=1)\n",
        "    current_model_basename = os.path.basename(current_model_path)\n",
        "    current_model_name = os.path.splitext(current_model_basename)[0]\n",
        "    test_result_df[\"test_prediction_\"+current_model_name] = test_predictions[:,1]\n",
        "    test_result_df[\"predicted_classes_\"+current_model_name] = predicted_classes"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665949350823
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the 5 models predictions on the independent test set from the 5-fold model\n",
        "test_result_df.to_csv(\"test_5fold_df.csv\",index=False)"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665949415705
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1665948387831
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}