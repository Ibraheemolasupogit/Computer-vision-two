{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 00_DICOM_to_NIFTI.ipynb\n",
        "\n",
        "This notebook convert all the CT scans in DICOM format (.dmc) to NIFTI format(.nii). \n",
        "\n",
        "Original DICOM files are located in the raw data folder (`raw_data`). The converted NIFTI files are saved in a new directory named `CT_data/CT_NIFTI_full`. \n",
        "\n",
        "P.S. `*/*/DICOM/*/*/*/*` is in the format of:\n",
        "\n",
        "`normal_pat/PatientName/DICOM/basename1/basename2/basename3/filename_3d`\n",
        "\n",
        "- `normal_pat`: abnormal / normal_pat\n",
        "- `PatientName`: Patient1 / Patient2 / ...\n",
        "- `DICOM`: DICOM / other patient information (only `DICOM` is considered)\n",
        "- `basename1`; `basename2`; `basename3`: Pseudo case names of the patient\n",
        "- `filename_3d`: Pseudo name of the CT scan"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "repo_dir = <PATH TO THIS REPO> # local path to this repo.\n",
        "sys.path.insert(0, repo_dir)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dicom2nifti\n",
        "import os\n",
        "import tqdm\n",
        "import glob\n",
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1665606622991
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Locating the folder paths used in this script.\n",
        "work_dir = <PATH TO project_dir> # path to the directory of the current project (project master path)\n",
        "data_path = work_dir + 'raw_data/' # path to original DICOM dataset\n",
        "target_path = work_dir + 'CT_data/' + \"CT_NIFTI_full/\" # path to the converted NIFTI dataset \n",
        "target_abnormal_path = target_path + \"abnormal/\" # path to the converted NIFTI dataset (abnormal scans)\n",
        "target_normal_path = target_path + \"normal/\" # path to the converted NIFTI dataset (normal scans)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1665606623055
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup the list to record any fail convertions.\n",
        "if os.path.exists(target_path+\"fail_list.csv\"):\n",
        "    # load the existed fail list and save it to a list\n",
        "    df_fail_orig = pd.read_csv(target_path+\"fail_list.csv\",index_col=[0])\n",
        "    fail_list = df_fail_orig.fail_scan.to_list()\n",
        "else:\n",
        "    # create a empty fail_list\n",
        "    fail_list = []"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converte the abnormal scans"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through all the abnormal cases\n",
        "for series_path in tqdm.tqdm(glob.glob(data_path + 'abnormal/*/DICOM/*/*/*/*')):\n",
        "    series = series_path\n",
        "    number_of_instance = len(os.listdir(series))\n",
        "    if number_of_instance > 3: # avoid converting localizer\n",
        "        try:\n",
        "            seriescode = os.path.basename(series)\n",
        "            seriesfilename = seriescode + \".nii\"\n",
        "            output_file = target_abnormal_path + seriesfilename\n",
        "            if os.path.isfile(output_file):\n",
        "                pass\n",
        "            else:\n",
        "                dicom2nifti.convert_dicom.dicom_series_to_nifti(series,output_file)\n",
        "        except:\n",
        "            print('Can not convert: ', seriescode)\n",
        "            fail_list.append(seriescode)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converte the normal scans"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through all the normal cases\n",
        "for series_path in tqdm.tqdm(glob.glob(data_path + 'normal/*/DICOM/*/*/*/*')):\n",
        "    series = series_path\n",
        "    number_of_instance = len(os.listdir(series))\n",
        "    if number_of_instance > 3: # avoid converting localizer\n",
        "        try:\n",
        "            seriescode = os.path.basename(series)\n",
        "            seriesfilename = seriescode + \".nii\"\n",
        "            output_file = target_normal_path + seriesfilename\n",
        "            if os.path.isfile(output_file):\n",
        "                pass\n",
        "            else:\n",
        "                dicom2nifti.convert_dicom.dicom_series_to_nifti(series,output_file)\n",
        "        except:\n",
        "            print('Can not convert: ', seriescode)\n",
        "            fail_list.append(seriescode)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1665607394502
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Record the fail scans\n",
        "Record which scans were failed during the convertion. Name of failed scan are saved in `fail_list.csv`."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record the fail list of files\n",
        "dict={\"fail_scan\":list(set(fail_list))}\n",
        "df_fail = pd.DataFrame(dict)\n",
        "df_fail.to_csv(target_path+\"fail_list.csv\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1665607539253
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}